{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import necessary modules**\n",
    "nn and optim for neural network construction and optimizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, cv2, os, random, json, time, sys\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import _LRScheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture\n",
    "* Individual Components:\n",
    "    * CBAM Module\n",
    "    * Channel Attention\n",
    "    * Spatial Attention\n",
    "    * DCNN (Deep Convolutional Neural Network)\n",
    "\n",
    "Convolutional Block Attention Module:\n",
    "This allows the neural network to focus on specific aspects of the image and improves\n",
    "the representation of interests. If, for example, the input feature map tensor dimensions\n",
    "are 6 x 127 x 127, the output will have the same dimensions. The CBAM module works on each feature map and enhances certain aspects of each feature map. \n",
    "\n",
    "# Proposed Architecture for this task**:\n",
    "**CNN Feature Extracion**\n",
    "Input (128 x 128) -> \n",
    "6-filter_Conv2d (output 6 x 126 x 126) (3 x 3 filter) ->\n",
    "CBAM (output 6 x 126 x 126) ->\n",
    "MaxPool (output 6 x 63 x 63) with padding ->\n",
    "12-filter_Conv2d (output 12 x 61 x 61) (3 x 3 filter) ->\n",
    "CBAM (output 12 x 61 x 61) ->\n",
    "MaxPool (output 12 x 31 x 31) with padding -> \n",
    "18-filter_Conv2d (output 18 x 29 x 29) (3 x 3 filter) ->\n",
    "MaxPool (output 18 x 15 x 15) with padding ->\n",
    "Flatten for NN classification (output 4050 x 1) ->\n",
    "DNN (output 5 x 1) (5 different classifications)\n",
    "\n",
    "**DNN Classification**\n",
    "L1 (4050) ->\n",
    "ReLU ->\n",
    "L2 (512) ->\n",
    "ReLU ->\n",
    "L3 (256) ->\n",
    "ReLU ->\n",
    "L4 (128) ->\n",
    "ReLU ->\n",
    "L5 (64) ->\n",
    "ReLU ->\n",
    "L6 (32) ->\n",
    "ReLU ->\n",
    "L7 (5) ->\n",
    "Softmax ->\n",
    "\n",
    "Output: Classification probabilities (5 x 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class channel_attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(channel_attention, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "class spatial_attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(spatial_attention, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CBAM, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DNN, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.feature_extraction = CNN()\n",
    "        self.classification = DNN()\n",
    "    def forward(self, x):\n",
    "        features = self.feature_extraction(x)\n",
    "        flattened_features = features.view(-1)\n",
    "        classification = self.classification(flattened_features)\n",
    "        return classification\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
